{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Conditional VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use ggplot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "def load_array(filename, task):\n",
    "    datapoint = np.load(filename)\n",
    "    if task == 'task 1':\n",
    "        initial_state = datapoint['initial_state']\n",
    "        terminal_state = datapoint['terminal_state']\n",
    "        return initial_state, terminal_state\n",
    "    elif task == 'task 2' or task == 'task 3':\n",
    "        whole_trajectory = datapoint['trajectory']\n",
    "        # change shape: (num_bodies, attributes, time) ->  num_bodies, time, attributes\n",
    "        whole_trajectory = np.swapaxes(whole_trajectory, 1, 2)\n",
    "        initial_state = whole_trajectory[:, 0]\n",
    "        target = whole_trajectory[:, 1:, 1:]  # drop the first timepoint (second dim) and mass (last dim) for the prediction task\n",
    "        return initial_state, target\n",
    "    else:\n",
    "        raise NotImplementedError(\"'task' argument should be 'task 1', 'task 2' or 'task 3'!\")\n",
    "\n",
    "#### Create adjacency matrix\n",
    "\n",
    "# Define distance metrics\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.sqrt(torch.sum((x - y)**2))\n",
    "\n",
    "def inverse_distance(x, y):\n",
    "    return 1 / euclidean_distance(x, y)\n",
    "\n",
    "# Create adjacency matrix function\n",
    "def create_adjacency_matrix(data, distance_metric):\n",
    "    n = data.shape[0]\n",
    "    adjacency_matrix = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:  # we don't calculate the distance of the object to itself\n",
    "                # we extract the position [x, y] for both objects i and j\n",
    "                position_i = data[i, 1:3]\n",
    "                position_j = data[j, 1:3]\n",
    "                adjacency_matrix[i, j] = distance_metric(position_i, position_j)\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Validate input\n",
    "def validate_input(X, adjacency_matrix):\n",
    "    # X should be a 2D tensor\n",
    "    assert X.dim() == 2, f\"X must be 2D, but got shape {X.shape}\"\n",
    "\n",
    "    # The number of nodes should be the same in X and the adjacency matrix\n",
    "    assert X.shape[0] == adjacency_matrix.shape[0] == adjacency_matrix.shape[1], \\\n",
    "        f\"Mismatch in number of nodes: got {X.shape[0]} nodes in X, but {adjacency_matrix.shape[0]} nodes in adjacency matrix\"\n",
    "\n",
    "    # The adjacency matrix should be square\n",
    "    assert adjacency_matrix.shape[0] == adjacency_matrix.shape[1], \\\n",
    "        f\"Adjacency matrix must be square, but got shape {adjacency_matrix.shape}\"\n",
    "\n",
    "    print(\"All checks passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" # NOTE: Define device here ONCE\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "\n",
    "# DataLoaders for task 2\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root, filenames, transform=None, pre_transform=None):\n",
    "        self.filenames = filenames\n",
    "        super(MyDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.filenames\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def get(self, idx):\n",
    "        X, y = load_array(self.filenames[idx], task='task 2')\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)   # y is now a 3D tensor\n",
    "\n",
    "        adjacency_matrix = create_adjacency_matrix(X, inverse_distance)\n",
    "        edge_index = adjacency_matrix.nonzero().t().contiguous().to(torch.long)\n",
    "\n",
    "        data = Data(x=X, y=y, edge_index=edge_index)  # y is now a 3D tensor\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "filenames = [f'data/task 2_3/train/trajectory_{i}.npz' for i in range(900)]\n",
    "split_point = int(len(filenames) * 0.8)\n",
    "# Do training validation split\n",
    "train_filenames = filenames[:split_point]\n",
    "val_filenames = filenames[split_point:]\n",
    "\n",
    "train_dataset = MyDataset(root='data/task 2_3/train', filenames=train_filenames)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "\n",
    "val_dataset = MyDataset(root='data/task 2_3/train', filenames=val_filenames)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# Prepare for validation data set\n",
    "\n",
    "test_filenames = [f'data/task 2_3/test/trajectory_{i}.npz' for i in range(901, 1000)]\n",
    "test_dataset = MyDataset(root='data/task 2_3/test', filenames=test_filenames)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[6, 5], edge_index=[2, 30], y=[6, 49, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print shape of first batch in train_dataloader\n",
    "for batch in train_dataloader:\n",
    "    graph = batch[3]\n",
    "    break\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels*2)\n",
    "        self.conv2 = SAGEConv(hidden_channels*2, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_channels, hidden_size=hidden_channels, batch_first=True)\n",
    "        \n",
    "        # Linear layers to compute mean and log variance of latent space\n",
    "        self.fc_mu = nn.Linear(hidden_channels, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_channels, latent_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # data has edge_index for graph structure\n",
    "        # trajectory has shape (n, 49, 4), where n is the number of objects, 49 is timesteps, and 4 is (x, y, velocity_x, velocity_y)\n",
    "        \n",
    "        x, edge_index = data.y , data.edge_index\n",
    "\n",
    "        # Graph convolution layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # LSTM layer to encode temporal information\n",
    "        # Assuming x now has shape (n, 49, hidden_channels) - one feature vector for each node at each timestep\n",
    "        lstm_outputs, _ = self.lstm(x)\n",
    "        final_output = lstm_outputs[:, -1, :]\n",
    "\n",
    "        # Linear layers for mean and log variance\n",
    "        mu = self.fc_mu(final_output)\n",
    "        log_var = self.fc_var(final_output)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "# Example usage:\n",
    "latent_dim = 16\n",
    "encoder = Encoder(num_features=4, hidden_channels=64, latent_dim=latent_dim)\n",
    "\n",
    "# Example usage:\n",
    "latent_dim = 16\n",
    "encoder = Encoder(num_features=4, hidden_channels=64, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional prior\n",
    "\n",
    "class MinGraphSAGE(torch.nn.Module):\n",
    "    # FINAL MODEL!\n",
    "    def __init__(self, num_features, hidden_channels, latent_dim):\n",
    "        super(MinGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels*2)\n",
    "        self.conv2 = SAGEConv(hidden_channels*2, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, latent_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # 1st GraphSAGE layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Droput\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 2nd GraphSAGE layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # 3rd GraphSAGE layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "latent_dim = 16\n",
    "min_graph_sage = MinGraphSAGE(num_features=5, hidden_channels=64, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # input_dim: dimension of the concatenated latent representation and conditioning vector\n",
    "        # hidden_dim: number of hidden units in the LSTM\n",
    "        # output_dim: dimension of the output at each timestep (4 in this case for x, y, vel_x, vel_y)\n",
    "        # num_layers: number of layers in the LSTM\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.num_timesteps = 49\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: concatenated latent representation and conditioning vectors of shape [n, 32]\n",
    "        \n",
    "        # Repeat x along a new temporal dimension to create a sequence, with the repeated\n",
    "        # x as the input at each time step. This is necessary because the LSTM expects\n",
    "        # input of the form (batch_size, sequence_length, input_size).\n",
    "        x = x.unsqueeze(1).repeat(1, self.num_timesteps, 1)\n",
    "        \n",
    "        # Pass the sequence through the LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out has shape [n, 49, hidden_dim]\n",
    "        \n",
    "        # Pass the LSTM output through a fully connected layer to get the final output\n",
    "        output = self.fc(lstm_out)  # output has shape [n, 49, 4]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "decoder = Decoder(input_dim=32, hidden_dim=64, output_dim=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for a trial run\n",
    "# Encode the trajectories \n",
    "# 16 dimensions for mu, logvar PER NODE\n",
    "mu, logvar = encoder(graph)\n",
    "\n",
    "# Encode the initial states (16 dimensions per node)\n",
    "conditioning_vector = min_graph_sage(graph)\n",
    "\n",
    "def reparameterize(mu, log_var):\n",
    "    # Reparameterization trick to sample from a Gaussian\n",
    "    # mu: mean matrix [n, 16]\n",
    "    # log_var: log variance matrix [n, 16]\n",
    "    std = torch.exp(0.5 * log_var)  # Standard deviation\n",
    "    eps = torch.randn_like(std)  # 'random' noise\n",
    "    return mu + eps * std\n",
    "\n",
    "# Sampling from the latent space\n",
    "z = reparameterize(mu, logvar)\n",
    "\n",
    "# Concatenate the sampled latent representation with the conditioning vector\n",
    "combined_features = torch.cat((z, conditioning_vector), dim=1)\n",
    "\n",
    "reconstructed = decoder(combined_features)\n",
    "original = graph.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 49, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final MODEL\n",
    "\n",
    "class GraphCVAE(torch.nn.Module):\n",
    "    def __init__(self, encoder, min_graph_sage, decoder):\n",
    "        super(GraphCVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.min_graph_sage = min_graph_sage\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def sample_latent(self, mu, log_var, use_prior=False):\n",
    "        if use_prior:\n",
    "            z = torch.randn_like(mu)\n",
    "        else:\n",
    "            std = torch.exp(0.5 * log_var)  \n",
    "            eps = torch.randn_like(std)  \n",
    "            z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, data, trajectory=None, is_inference=False):\n",
    "        if is_inference:\n",
    "            # In inference mode, encode only the initial state into conditioning vector\n",
    "            conditioning_vector = self.min_graph_sage(data=data)\n",
    "            z = torch.randn((data.num_nodes, 16)).to(conditioning_vector.device)\n",
    "        else:\n",
    "            # In training mode, encode both the initial state and the full trajectory\n",
    "            mu, log_var = self.encoder(data=data)\n",
    "            conditioning_vector = self.min_graph_sage(data=data)\n",
    "            z = self.sample_latent(mu, log_var)\n",
    "\n",
    "        # Concatenate the latent representation and conditioning vector\n",
    "        combined_features = torch.cat((z, conditioning_vector), dim=1)\n",
    "        \n",
    "        # Pass the combined features to the decoder\n",
    "        output_sequence = self.decoder(combined_features)\n",
    "\n",
    "        return output_sequence\n",
    "\n",
    "\n",
    "# Initialize components\n",
    "encoder = Encoder(num_features=4, hidden_channels=16, latent_dim=16)\n",
    "min_graph_sage = MinGraphSAGE(num_features=5, hidden_channels=16, latent_dim=16)\n",
    "decoder = Decoder(input_dim=32, hidden_dim=64, output_dim=4)\n",
    "\n",
    "# Initialize model\n",
    "graph_cvae = GraphCVAE(encoder, min_graph_sage, decoder)\n",
    "graph_cvae(graph, is_inference=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average loss: 4834.6256, Validation loss: 3012.5416\n",
      "====> Epoch: 1 Average loss: 4255.2876, Validation loss: 3696.2077\n",
      "====> Epoch: 2 Average loss: 4267.2710, Validation loss: 3924.9832\n",
      "====> Epoch: 3 Average loss: 4720.8010, Validation loss: 4587.9356\n",
      "====> Epoch: 4 Average loss: 4931.4741, Validation loss: 4179.8552\n",
      "====> Epoch: 5 Average loss: 4529.7283, Validation loss: 4508.9857\n",
      "====> Epoch: 6 Average loss: 6181.6733, Validation loss: 4790.1939\n",
      "====> Epoch: 7 Average loss: 5705.9921, Validation loss: 5001.0549\n",
      "====> Epoch: 8 Average loss: 6337.1200, Validation loss: 5230.5267\n",
      "====> Epoch: 9 Average loss: 5412.9052, Validation loss: 5728.8489\n",
      "====> Epoch: 10 Average loss: 6081.0069, Validation loss: 4964.9698\n",
      "====> Epoch: 11 Average loss: 7334.0022, Validation loss: 8674.6104\n",
      "====> Epoch: 12 Average loss: 7993.9612, Validation loss: 5331.7389\n",
      "====> Epoch: 13 Average loss: 5657.9289, Validation loss: 4193.4042\n",
      "====> Epoch: 14 Average loss: 5098.0322, Validation loss: 4493.1299\n",
      "====> Epoch: 15 Average loss: 4822.6800, Validation loss: 3830.5624\n",
      "====> Epoch: 16 Average loss: 4832.2517, Validation loss: 5293.1147\n",
      "====> Epoch: 17 Average loss: 6220.1694, Validation loss: 4432.6049\n",
      "====> Epoch: 18 Average loss: 5805.5214, Validation loss: 3847.4001\n",
      "====> Epoch: 19 Average loss: 4471.1043, Validation loss: 3645.1699\n",
      "====> Epoch: 20 Average loss: 4461.8631, Validation loss: 3631.9351\n",
      "====> Epoch: 21 Average loss: 5020.5561, Validation loss: 4739.4871\n",
      "====> Epoch: 22 Average loss: 6632.5113, Validation loss: 5763.2234\n",
      "====> Epoch: 23 Average loss: 5823.5407, Validation loss: 5987.3371\n",
      "====> Epoch: 24 Average loss: 7001.6146, Validation loss: 6476.0026\n",
      "====> Epoch: 25 Average loss: 5871.4018, Validation loss: 4146.0409\n",
      "====> Epoch: 26 Average loss: 5007.8515, Validation loss: 4197.2170\n",
      "====> Epoch: 27 Average loss: 4751.8433, Validation loss: 3811.0438\n",
      "====> Epoch: 28 Average loss: 4832.2070, Validation loss: 4103.3391\n",
      "====> Epoch: 29 Average loss: 5585.9611, Validation loss: 4429.2065\n",
      "====> Epoch: 30 Average loss: 5971.9118, Validation loss: 5792.5943\n",
      "====> Epoch: 31 Average loss: 7596.2446, Validation loss: 4970.4809\n",
      "====> Epoch: 32 Average loss: 5667.2127, Validation loss: 4303.2834\n",
      "====> Epoch: 33 Average loss: 4954.5963, Validation loss: 4153.9756\n",
      "====> Epoch: 34 Average loss: 4668.1116, Validation loss: 3785.4356\n",
      "====> Epoch: 35 Average loss: 4622.3964, Validation loss: 3852.1542\n",
      "====> Epoch: 36 Average loss: 5224.3222, Validation loss: 4638.7438\n",
      "====> Epoch: 37 Average loss: 5332.7585, Validation loss: 4055.0353\n",
      "====> Epoch: 38 Average loss: 4990.2025, Validation loss: 4069.7300\n",
      "====> Epoch: 39 Average loss: 5123.3478, Validation loss: 4227.6833\n",
      "====> Epoch: 40 Average loss: 5339.6258, Validation loss: 4931.8363\n",
      "====> Epoch: 41 Average loss: 5380.4555, Validation loss: 5709.3782\n",
      "====> Epoch: 42 Average loss: 5572.1454, Validation loss: 4678.4982\n",
      "====> Epoch: 43 Average loss: 5896.5144, Validation loss: 5554.7156\n",
      "====> Epoch: 44 Average loss: 6364.4380, Validation loss: 6942.6343\n",
      "====> Epoch: 45 Average loss: 5704.3554, Validation loss: 4571.6921\n",
      "====> Epoch: 46 Average loss: 5310.8803, Validation loss: 4179.5447\n",
      "====> Epoch: 47 Average loss: 5132.2014, Validation loss: 5126.2926\n",
      "====> Epoch: 48 Average loss: 6193.6593, Validation loss: 5376.4736\n",
      "====> Epoch: 49 Average loss: 6044.6082, Validation loss: 5098.8409\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "# NOTE: Training only works on single graphs, not batches of graphs\n",
    "# Due to this, training is very slow, and not necessarily converges\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Loss functions\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    MSE = torch.nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "\n",
    "# Training settings\n",
    "epochs = 50\n",
    "learning_rate = 0.01 \n",
    "\n",
    "# Initialize components\n",
    "encoder = Encoder(num_features=4, hidden_channels=16, latent_dim=16)\n",
    "min_graph_sage = MinGraphSAGE(num_features=5, hidden_channels=16, latent_dim=16)\n",
    "decoder = Decoder(input_dim=32, hidden_dim=64, output_dim=4)\n",
    "\n",
    "# Initialize model\n",
    "model = GraphCVAE(encoder, min_graph_sage, decoder)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Training\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch[0]\n",
    "        # Move data to the device (CPU or GPU)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        recon_batch = model(data=batch, trajectory=batch.y)\n",
    "\n",
    "        # Loss calculation\n",
    "        mu, log_var = model.encoder(data=batch)\n",
    "        loss = loss_function(recon_batch, batch.y, mu, log_var)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = batch[0]\n",
    "            # Move data to the device (CPU or GPU)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            recon_batch = model(data=batch, trajectory=batch.y)\n",
    "\n",
    "            # Loss calculation\n",
    "            mu, log_var = model.encoder(data=batch)\n",
    "            loss = loss_function(recon_batch, batch.y, mu, log_var)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Logging\n",
    "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_dataloader.dataset):.4f}, '\n",
    "          f'Validation loss: {val_loss / len(val_dataloader.dataset):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
