{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Conditional VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use ggplot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "def load_array(filename, task):\n",
    "    datapoint = np.load(filename)\n",
    "    if task == 'task 1':\n",
    "        initial_state = datapoint['initial_state']\n",
    "        terminal_state = datapoint['terminal_state']\n",
    "        return initial_state, terminal_state\n",
    "    elif task == 'task 2' or task == 'task 3':\n",
    "        whole_trajectory = datapoint['trajectory']\n",
    "        # change shape: (num_bodies, attributes, time) ->  num_bodies, time, attributes\n",
    "        whole_trajectory = np.swapaxes(whole_trajectory, 1, 2)\n",
    "        initial_state = whole_trajectory[:, 0]\n",
    "        target = whole_trajectory[:, 1:, 1:]  # drop the first timepoint (second dim) and mass (last dim) for the prediction task\n",
    "        return initial_state, target\n",
    "    else:\n",
    "        raise NotImplementedError(\"'task' argument should be 'task 1', 'task 2' or 'task 3'!\")\n",
    "\n",
    "#### Create adjacency matrix\n",
    "\n",
    "# Define distance metrics\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.sqrt(torch.sum((x - y)**2))\n",
    "\n",
    "def inverse_distance(x, y):\n",
    "    return 1 / euclidean_distance(x, y)\n",
    "\n",
    "# Create adjacency matrix function\n",
    "def create_adjacency_matrix(data, distance_metric):\n",
    "    n = data.shape[0]\n",
    "    adjacency_matrix = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:  # we don't calculate the distance of the object to itself\n",
    "                # we extract the position [x, y] for both objects i and j\n",
    "                position_i = data[i, 1:3]\n",
    "                position_j = data[j, 1:3]\n",
    "                adjacency_matrix[i, j] = distance_metric(position_i, position_j)\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Validate input\n",
    "def validate_input(X, adjacency_matrix):\n",
    "    # X should be a 2D tensor\n",
    "    assert X.dim() == 2, f\"X must be 2D, but got shape {X.shape}\"\n",
    "\n",
    "    # The number of nodes should be the same in X and the adjacency matrix\n",
    "    assert X.shape[0] == adjacency_matrix.shape[0] == adjacency_matrix.shape[1], \\\n",
    "        f\"Mismatch in number of nodes: got {X.shape[0]} nodes in X, but {adjacency_matrix.shape[0]} nodes in adjacency matrix\"\n",
    "\n",
    "    # The adjacency matrix should be square\n",
    "    assert adjacency_matrix.shape[0] == adjacency_matrix.shape[1], \\\n",
    "        f\"Adjacency matrix must be square, but got shape {adjacency_matrix.shape}\"\n",
    "\n",
    "    print(\"All checks passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" # NOTE: Define device here ONCE\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "\n",
    "# DataLoaders for task 2\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root, filenames, transform=None, pre_transform=None):\n",
    "        self.filenames = filenames\n",
    "        super(MyDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.filenames\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def get(self, idx):\n",
    "        X, y = load_array(self.filenames[idx], task='task 2')\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)   # y is now a 3D tensor\n",
    "\n",
    "        adjacency_matrix = create_adjacency_matrix(X, inverse_distance)\n",
    "        edge_index = adjacency_matrix.nonzero().t()\n",
    "\n",
    "        data = Data(x=X, y=y, edge_index=edge_index)  # y is now a 3D tensor\n",
    "\n",
    "        return data\n",
    "\n",
    "filenames = [f'data/task 2_3/train/trajectory_{i}.npz' for i in range(900)]\n",
    "split_point = int(len(filenames) * 0.8)\n",
    "# Do training validation split\n",
    "train_filenames = filenames[:split_point]\n",
    "val_filenames = filenames[split_point:]\n",
    "\n",
    "train_dataset = MyDataset(root='data/task 2_3/train', filenames=train_filenames)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "val_dataset = MyDataset(root='data/task 2_3/train', filenames=val_filenames)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Prepare for validation data set\n",
    "\n",
    "test_filenames = [f'data/task 2_3/test/trajectory_{i}.npz' for i in range(901, 1000)]\n",
    "test_dataset = MyDataset(root='data/task 2_3/test', filenames=test_filenames)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[6, 5], edge_index=[2, 30], y=[6, 49, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print shape of first batch in train_dataloader\n",
    "for batch in train_dataloader:\n",
    "    graph = batch[3]\n",
    "    break\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels*2)\n",
    "        self.conv2 = SAGEConv(hidden_channels*2, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_channels, hidden_size=hidden_channels, batch_first=True)\n",
    "        \n",
    "        # Linear layers to compute mean and log variance of latent space\n",
    "        self.fc_mu = nn.Linear(hidden_channels, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_channels, latent_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.y, data.edge_index, data.batch\n",
    "\n",
    "        # Handle batches\n",
    "        if batch is not None:\n",
    "            cumsum = torch.cat([torch.tensor([0], dtype=torch.long, device=edge_index.device), batch.bincount().cumsum(0)[:-1]])\n",
    "            edge_index += cumsum[edge_index[0]]\n",
    "\n",
    "        # Graph convolution layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # LSTM layer to encode temporal information\n",
    "        lstm_outputs, _ = self.lstm(x)\n",
    "        final_output = lstm_outputs[:, -1, :]\n",
    "\n",
    "        # Linear layers for mean and log variance\n",
    "        mu = self.fc_mu(final_output)\n",
    "        log_var = self.fc_var(final_output)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "# Example usage:\n",
    "latent_dim = 16\n",
    "encoder = Encoder(num_features=4, hidden_channels=64, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional prior\n",
    "\n",
    "class MinGraphSAGE(torch.nn.Module):\n",
    "    # FINAL MODEL!\n",
    "    def __init__(self, num_features, hidden_channels, latent_dim):\n",
    "        super(MinGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels*2)\n",
    "        self.conv2 = SAGEConv(hidden_channels*2, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, latent_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # 1st GraphSAGE layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Droput\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 2nd GraphSAGE layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # 3rd GraphSAGE layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "latent_dim = 16\n",
    "min_graph_sage = MinGraphSAGE(num_features=5, hidden_channels=64, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # input_dim: dimension of the concatenated latent representation and conditioning vector\n",
    "        # hidden_dim: number of hidden units in the LSTM\n",
    "        # output_dim: dimension of the output at each timestep (4 in this case for x, y, vel_x, vel_y)\n",
    "        # num_layers: number of layers in the LSTM\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.num_timesteps = 49\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: concatenated latent representation and conditioning vectors of shape [n, 32]\n",
    "        \n",
    "        # Repeat x along a new temporal dimension to create a sequence, with the repeated\n",
    "        # x as the input at each time step. This is necessary because the LSTM expects\n",
    "        # input of the form (batch_size, sequence_length, input_size).\n",
    "        x = x.unsqueeze(1).repeat(1, self.num_timesteps, 1)\n",
    "        \n",
    "        # Pass the sequence through the LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out has shape [n, 49, hidden_dim]\n",
    "        \n",
    "        # Pass the LSTM output through a fully connected layer to get the final output\n",
    "        output = self.fc(lstm_out)  # output has shape [n, 49, 4]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "decoder = Decoder(input_dim=32, hidden_dim=64, output_dim=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for a trial run\n",
    "# Encode the trajectories \n",
    "# 16 dimensions for mu, logvar PER NODE\n",
    "mu, logvar = encoder(graph)\n",
    "\n",
    "# Encode the initial states (16 dimensions per node)\n",
    "conditioning_vector = min_graph_sage(graph)\n",
    "\n",
    "def reparameterize(mu, log_var):\n",
    "    # Reparameterization trick to sample from a Gaussian\n",
    "    # mu: mean matrix [n, 16]\n",
    "    # log_var: log variance matrix [n, 16]\n",
    "    std = torch.exp(0.5 * log_var)  # Standard deviation\n",
    "    eps = torch.randn_like(std)  # 'random' noise\n",
    "    return mu + eps * std\n",
    "\n",
    "# Sampling from the latent space\n",
    "z = reparameterize(mu, logvar)\n",
    "\n",
    "# Concatenate the sampled latent representation with the conditioning vector\n",
    "combined_features = torch.cat((z, conditioning_vector), dim=1)\n",
    "\n",
    "reconstructed = decoder(combined_features)\n",
    "original = graph.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 32 is out of bounds for dimension 0 with size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 16 dimensions for mu, logvar PER NODE\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mu, logvar \u001b[39m=\u001b[39m encoder(batch)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Encode the initial states (16 dimensions per node)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# conditioning_vector = min_graph_sage(graph)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreparameterize\u001b[39m(mu, log_var):\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Reparameterization trick to sample from a Gaussian\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m# mu: mean matrix [n, 16]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# log_var: log variance matrix [n, 16]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[64], line 29\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     cumsum \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice), batch\u001b[39m.\u001b[39mbincount()\u001b[39m.\u001b[39mcumsum(\u001b[39m0\u001b[39m)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n\u001b[0;32m---> 29\u001b[0m     edge_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cumsum[edge_index[\u001b[39m0\u001b[39;49m]]\n\u001b[1;32m     31\u001b[0m \u001b[39m# Graph convolution layers\u001b[39;00m\n\u001b[1;32m     32\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x, edge_index)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 32 is out of bounds for dimension 0 with size 32"
     ]
    }
   ],
   "source": [
    "# 16 dimensions for mu, logvar PER NODE\n",
    "mu, logvar = encoder(batch)\n",
    "\n",
    "# Encode the initial states (16 dimensions per node)\n",
    "# conditioning_vector = min_graph_sage(graph)\n",
    "\n",
    "def reparameterize(mu, log_var):\n",
    "    # Reparameterization trick to sample from a Gaussian\n",
    "    # mu: mean matrix [n, 16]\n",
    "    # log_var: log variance matrix [n, 16]\n",
    "    std = torch.exp(0.5 * log_var)  # Standard deviation\n",
    "    eps = torch.randn_like(std)  # 'random' noise\n",
    "    return mu + eps * std\n",
    "\n",
    "# Sampling from the latent space\n",
    "# z = reparameterize(mu, logvar)\n",
    "\n",
    "# Concatenate the sampled latent representation with the conditioning vector\n",
    "# combined_features = torch.cat((z, conditioning_vector), dim=1)\n",
    "\n",
    "# reconstructed = decoder(combined_features)\n",
    "# original = graph.y\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 49, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final MODEL\n",
    "\n",
    "class GraphCVAE(torch.nn.Module):\n",
    "    def __init__(self, encoder, min_graph_sage, decoder):\n",
    "        super(GraphCVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.min_graph_sage = min_graph_sage\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def sample_latent(self, mu, log_var, use_prior=False):\n",
    "        if use_prior:\n",
    "            z = torch.randn_like(mu)\n",
    "        else:\n",
    "            std = torch.exp(0.5 * log_var)  \n",
    "            eps = torch.randn_like(std)  \n",
    "            z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, data, trajectory=None, is_inference=False):\n",
    "        if is_inference:\n",
    "            # In inference mode, encode only the initial state into conditioning vector\n",
    "            conditioning_vector = self.min_graph_sage(data=data)\n",
    "            z = torch.randn((data.num_nodes, 16)).to(conditioning_vector.device)\n",
    "        else:\n",
    "            # In training mode, encode both the initial state and the full trajectory\n",
    "            mu, log_var = self.encoder(data=data)\n",
    "            conditioning_vector = self.min_graph_sage(data=data)\n",
    "            z = self.sample_latent(mu, log_var)\n",
    "\n",
    "        # Concatenate the latent representation and conditioning vector\n",
    "        combined_features = torch.cat((z, conditioning_vector), dim=1)\n",
    "        \n",
    "        # Pass the combined features to the decoder\n",
    "        output_sequence = self.decoder(combined_features)\n",
    "\n",
    "        return output_sequence\n",
    "\n",
    "\n",
    "# Initialize components\n",
    "encoder = Encoder(num_features=4, hidden_channels=16, latent_dim=16)\n",
    "min_graph_sage = MinGraphSAGE(num_features=5, hidden_channels=16, latent_dim=16)\n",
    "decoder = Decoder(input_dim=32, hidden_dim=64, output_dim=4)\n",
    "\n",
    "# Initialize model\n",
    "graph_cvae = GraphCVAE(encoder, min_graph_sage, decoder)\n",
    "graph_cvae(graph, is_inference=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Encountered an index error. Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 48] (got interval [0, 7103])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:272\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    271\u001b[0m     index \u001b[39m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mindex_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim, index)\n\u001b[1;32m    273\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: INDICES element is out of DATA bounds, id=230 axis_dim=49",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m recon_batch \u001b[39m=\u001b[39m model(data\u001b[39m=\u001b[39;49mbatch, trajectory\u001b[39m=\u001b[39;49mbatch\u001b[39m.\u001b[39;49my)\n\u001b[1;32m     49\u001b[0m \u001b[39m# Loss calculation\u001b[39;00m\n\u001b[1;32m     50\u001b[0m mu, log_var \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder(data\u001b[39m=\u001b[39mbatch, trajectory\u001b[39m=\u001b[39mbatch\u001b[39m.\u001b[39my)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[59], line 26\u001b[0m, in \u001b[0;36mGraphCVAE.forward\u001b[0;34m(self, data, trajectory, is_inference)\u001b[0m\n\u001b[1;32m     23\u001b[0m     z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((data\u001b[39m.\u001b[39mnum_nodes, \u001b[39m16\u001b[39m))\u001b[39m.\u001b[39mto(conditioning_vector\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39m# In training mode, encode both the initial state and the full trajectory\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     mu, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(data\u001b[39m=\u001b[39;49mdata)\n\u001b[1;32m     27\u001b[0m     conditioning_vector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_graph_sage(data\u001b[39m=\u001b[39mdata)\n\u001b[1;32m     28\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_latent(mu, log_var)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[57], line 38\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     35\u001b[0m     edge_index \u001b[39m=\u001b[39m edge_index \u001b[39m+\u001b[39m data\u001b[39m.\u001b[39mbatch[edge_index[\u001b[39m0\u001b[39m]] \u001b[39m*\u001b[39m data\u001b[39m.\u001b[39mnum_nodes\n\u001b[1;32m     37\u001b[0m \u001b[39m# Graph convolution layers\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index)\n\u001b[1;32m     39\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch_geometric/nn/conv/sage_conv.py:131\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    128\u001b[0m     x \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mrelu(), x[\u001b[39m1\u001b[39m])\n\u001b[1;32m    130\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    132\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_l(out)\n\u001b[1;32m    134\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:459\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m decomp_args:\n\u001b[1;32m    457\u001b[0m         kwargs[arg] \u001b[39m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 459\u001b[0m coll_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_user_args, edge_index, size,\n\u001b[1;32m    460\u001b[0m                           kwargs)\n\u001b[1;32m    462\u001b[0m msg_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[1;32m    463\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_pre_hooks\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:336\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    335\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_size(size, dim, data)\n\u001b[0;32m--> 336\u001b[0m             data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lift(data, edge_index, dim)\n\u001b[1;32m    338\u001b[0m         out[arg] \u001b[39m=\u001b[39m data\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:275\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mmin() \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m index\u001b[39m.\u001b[39mmax() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m src\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim):\n\u001b[0;32m--> 275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[1;32m    276\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEncountered an index error. Please ensure that all \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindices in \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m point to valid indices in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe interval [0, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim)\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(got interval \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmin())\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax())\u001b[39m}\u001b[39;00m\u001b[39m])\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;31mIndexError\u001b[0m: Encountered an index error. Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 48] (got interval [0, 7103])"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Loss functions\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    MSE = torch.nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "\n",
    "# Training settings\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize components\n",
    "encoder = Encoder(num_features=4, hidden_channels=16, latent_dim=16)\n",
    "min_graph_sage = MinGraphSAGE(num_features=5, hidden_channels=16, latent_dim=16)\n",
    "decoder = Decoder(input_dim=32, hidden_dim=64, output_dim=4)\n",
    "\n",
    "# Initialize model\n",
    "model = GraphCVAE(encoder, min_graph_sage, decoder)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Training\n",
    "    for batch in train_dataloader:\n",
    "        # Move data to the device (CPU or GPU)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        recon_batch = model(data=batch, trajectory=batch.y)\n",
    "\n",
    "        # Loss calculation\n",
    "        mu, log_var = model.encoder(data=batch, trajectory=batch.y)\n",
    "        loss = loss_function(recon_batch, batch.y, mu, log_var)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # Move data to the device (CPU or GPU)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            recon_batch = model(data=batch, trajectory=batch.y)\n",
    "\n",
    "            # Loss calculation\n",
    "            mu, log_var = model.encoder(data=batch, trajectory=batch.y)\n",
    "            loss = loss_function(recon_batch, batch.y, mu, log_var)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Logging\n",
    "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_dataloader.dataset):.4f}, '\n",
    "          f'Validation loss: {val_loss / len(val_dataloader.dataset):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
