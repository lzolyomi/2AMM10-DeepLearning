{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_array(filename, task):\n",
    "    datapoint = np.load(filename)\n",
    "    if task == 'task 1':\n",
    "        initial_state = datapoint['initial_state']\n",
    "        terminal_state = datapoint['terminal_state']\n",
    "        return initial_state, terminal_state\n",
    "    elif task == 'task 2' or task == 'task 3':\n",
    "        whole_trajectory = datapoint['trajectory']\n",
    "        # change shape: (num_bodies, attributes, time) ->  num_bodies, time, attributes\n",
    "        whole_trajectory = np.swapaxes(whole_trajectory, 1, 2)\n",
    "        initial_state = whole_trajectory[:, 0]\n",
    "        target = whole_trajectory[:, 1:, 1:]  # drop the first timepoint (second dim) and mass (last dim) for the prediction task\n",
    "        return initial_state, target\n",
    "    else:\n",
    "        raise NotImplementedError(\"'task' argument should be 'task 1', 'task 2' or 'task 3'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create adjacency matrix\n",
    "\n",
    "# Define distance metrics\n",
    "def euclidean_distance(x, y):\n",
    "    return torch.sqrt(torch.sum((x - y)**2))\n",
    "\n",
    "def inverse_distance(x, y):\n",
    "    return 1 / euclidean_distance(x, y)\n",
    "\n",
    "# Create adjacency matrix function\n",
    "def create_adjacency_matrix(data, distance_metric):\n",
    "    n = data.shape[0]\n",
    "    adjacency_matrix = torch.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:  # we don't calculate the distance of the object to itself\n",
    "                # we extract the position [x, y] for both objects i and j\n",
    "                position_i = data[i, 1:3]\n",
    "                position_j = data[j, 1:3]\n",
    "                adjacency_matrix[i, j] = distance_metric(position_i, position_j)\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Validate input\n",
    "def validate_input(X, adjacency_matrix):\n",
    "    # X should be a 2D tensor\n",
    "    assert X.dim() == 2, f\"X must be 2D, but got shape {X.shape}\"\n",
    "\n",
    "    # The number of nodes should be the same in X and the adjacency matrix\n",
    "    assert X.shape[0] == adjacency_matrix.shape[0] == adjacency_matrix.shape[1], \\\n",
    "        f\"Mismatch in number of nodes: got {X.shape[0]} nodes in X, but {adjacency_matrix.shape[0]} nodes in adjacency matrix\"\n",
    "\n",
    "    # The adjacency matrix should be square\n",
    "    assert adjacency_matrix.shape[0] == adjacency_matrix.shape[1], \\\n",
    "        f\"Adjacency matrix must be square, but got shape {adjacency_matrix.shape}\"\n",
    "\n",
    "    print(\"All checks passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine sample data\n",
    "\"\"\"\n",
    "This cell gives an example of loading a datapoint with numpy for task 1.\n",
    "\n",
    "The arrays returned by the function are structures as follows:\n",
    "initial_state(X): shape (n_bodies, [mass, x, y, v_x, v_y])\n",
    "terminal_state(y): shape (n_bodies, [x, y])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "X, y= load_array('data/task 1/train/trajectory_0.npz', task='task 1')\n",
    "X = torch.tensor(X, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = create_adjacency_matrix(X, inverse_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,\n",
       "          3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "          6, 7, 7, 7, 7, 7, 7, 7],\n",
       "         [1, 2, 3, 4, 5, 6, 7, 0, 2, 3, 4, 5, 6, 7, 0, 1, 3, 4, 5, 6, 7, 0, 1, 2,\n",
       "          4, 5, 6, 7, 0, 1, 2, 3, 5, 6, 7, 0, 1, 2, 3, 4, 6, 7, 0, 1, 2, 3, 4, 5,\n",
       "          7, 0, 1, 2, 3, 4, 5, 6]]),\n",
       " tensor([[0.0000, 0.0918, 3.3833, 0.3152, 0.2411, 0.1777, 0.0905, 0.1391],\n",
       "         [0.0918, 0.0000, 0.0915, 0.0879, 0.1365, 0.1583, 0.0630, 0.2146],\n",
       "         [3.3833, 0.0915, 0.0000, 0.2885, 0.2323, 0.1807, 0.0930, 0.1402],\n",
       "         [0.3152, 0.0879, 0.2885, 0.0000, 0.2457, 0.1319, 0.0703, 0.1143],\n",
       "         [0.2411, 0.1365, 0.2323, 0.2457, 0.0000, 0.2297, 0.0743, 0.1993],\n",
       "         [0.1777, 0.1583, 0.1807, 0.1319, 0.2297, 0.0000, 0.0975, 0.5669],\n",
       "         [0.0905, 0.0630, 0.0930, 0.0703, 0.0743, 0.0975, 0.0000, 0.0874],\n",
       "         [0.1391, 0.2146, 0.1402, 0.1143, 0.1993, 0.5669, 0.0874, 0.0000]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test network\n",
    "import torch\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.conv1 = GraphConv(num_features, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # 1st Graph Convolution layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # 2nd Graph Convolution layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Convert adjacency matrix to edge_index\n",
    "# WARNING! This will 'Delete' the distances between the nodes\n",
    "edge_index = adjacency_matrix.nonzero().t()\n",
    "\n",
    "# Create a Data object\n",
    "data = Data(x=X, edge_index=edge_index)\n",
    "\n",
    "# Create an instance of our GNN\n",
    "model = SimpleGNN(num_features=5, hidden_channels=32, num_classes=2)\n",
    "\n",
    "# Pass the graph through the model\n",
    "out = model(data)\n",
    "\n",
    "print(out.shape)\n",
    "edge_index, adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Zoo\n",
    "\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels*2)\n",
    "        self.conv2 = SAGEConv(hidden_channels*2, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # 1st GraphSAGE layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # 2nd GraphSAGE layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "        # 3rd GraphSAGE layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root, filenames, transform=None, pre_transform=None):\n",
    "        self.filenames = filenames\n",
    "        super(MyDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return self.filenames\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def get(self, idx):\n",
    "        X, y = load_array(self.filenames[idx], task='task 1')\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        adjacency_matrix = create_adjacency_matrix(X, inverse_distance)\n",
    "        edge_index = adjacency_matrix.nonzero().t()\n",
    "\n",
    "        data = Data(x=X, y=y, edge_index=edge_index)\n",
    "\n",
    "        return data\n",
    "\n",
    "filenames = [f'data/task 1/train/trajectory_{i}.npz' for i in range(900)]\n",
    "dataset = MyDataset(root='data/task 1/train', filenames=filenames)\n",
    "\n",
    "# Prepare for validation data set\n",
    "\n",
    "val_filenames = [f'data/task 1/test/trajectory_{i}.npz' for i in range(901, 1000)]\n",
    "val_dataset = MyDataset(root='data/task 1/test', filenames=val_filenames)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 19.18354034423828, Test Loss: 15.109752529799335\n",
      "Epoch: 2, Training Loss: 12.629542350769043, Test Loss: 8.984456611402107\n",
      "Epoch: 3, Training Loss: 8.390835762023926, Test Loss: 5.331993970003995\n",
      "Epoch: 4, Training Loss: 7.516866207122803, Test Loss: 5.008034535128661\n",
      "Epoch: 5, Training Loss: 7.09335470199585, Test Loss: 4.8860001636273935\n",
      "Epoch: 6, Training Loss: 6.73557710647583, Test Loss: 4.799153523011641\n",
      "Epoch: 7, Training Loss: 6.436294078826904, Test Loss: 4.731423467096656\n",
      "Epoch: 8, Training Loss: 6.156905651092529, Test Loss: 4.674902643820252\n",
      "Epoch: 9, Training Loss: 5.91514253616333, Test Loss: 4.628289037280613\n",
      "Epoch: 10, Training Loss: 5.67647647857666, Test Loss: 4.598819756748701\n",
      "Epoch: 11, Training Loss: 5.492270469665527, Test Loss: 4.579380160630351\n",
      "Epoch: 12, Training Loss: 5.3515448570251465, Test Loss: 4.565064478402186\n",
      "Epoch: 13, Training Loss: 5.210579872131348, Test Loss: 4.552286569518272\n",
      "Epoch: 14, Training Loss: 5.0843987464904785, Test Loss: 4.539269396753022\n",
      "Epoch: 15, Training Loss: 4.968835830688477, Test Loss: 4.525125501131771\n",
      "Epoch: 16, Training Loss: 4.853538990020752, Test Loss: 4.515903981045039\n",
      "Epoch: 17, Training Loss: 4.758509635925293, Test Loss: 4.50283476078149\n",
      "Epoch: 18, Training Loss: 4.638915538787842, Test Loss: 4.48984560099515\n",
      "Epoch: 19, Training Loss: 4.579632759094238, Test Loss: 4.473782813910282\n",
      "Epoch: 20, Training Loss: 4.5022125244140625, Test Loss: 4.465744444818208\n",
      "Epoch: 21, Training Loss: 4.411372184753418, Test Loss: 4.453120026925598\n",
      "Epoch: 22, Training Loss: 4.332310199737549, Test Loss: 4.441534540869973\n",
      "Epoch: 23, Training Loss: 4.269703388214111, Test Loss: 4.430124227446739\n",
      "Epoch: 24, Training Loss: 4.216999530792236, Test Loss: 4.418656510536117\n",
      "Epoch: 25, Training Loss: 4.15635871887207, Test Loss: 4.412993139690823\n",
      "Epoch: 26, Training Loss: 4.118685245513916, Test Loss: 4.403808591341732\n",
      "Epoch: 27, Training Loss: 4.0770583152771, Test Loss: 4.396835710063125\n",
      "Epoch: 28, Training Loss: 4.036219120025635, Test Loss: 4.386003677291099\n",
      "Epoch: 29, Training Loss: 3.997300624847412, Test Loss: 4.377991442728525\n",
      "Epoch: 30, Training Loss: 3.975471258163452, Test Loss: 4.362066555504847\n",
      "Epoch: 31, Training Loss: 3.933431386947632, Test Loss: 4.353204818687054\n",
      "Epoch: 32, Training Loss: 3.9165451526641846, Test Loss: 4.332414891984728\n",
      "Epoch: 33, Training Loss: 3.8687918186187744, Test Loss: 4.323807742860582\n",
      "Epoch: 34, Training Loss: 3.843905210494995, Test Loss: 4.310759216848046\n",
      "Epoch: 35, Training Loss: 3.830894947052002, Test Loss: 4.2987243238121575\n",
      "Epoch: 36, Training Loss: 3.8004486560821533, Test Loss: 4.283315641711456\n",
      "Epoch: 37, Training Loss: 3.762725830078125, Test Loss: 4.272462081427526\n",
      "Epoch: 38, Training Loss: 3.7434327602386475, Test Loss: 4.254776824604381\n",
      "Epoch: 39, Training Loss: 3.7250747680664062, Test Loss: 4.24965018937082\n",
      "Epoch: 40, Training Loss: 3.6718318462371826, Test Loss: 4.239158656862047\n",
      "Epoch: 41, Training Loss: 3.6447219848632812, Test Loss: 4.223802289577446\n",
      "Epoch: 42, Training Loss: 3.60640549659729, Test Loss: 4.215326022620153\n",
      "Epoch: 43, Training Loss: 3.5755250453948975, Test Loss: 4.19871530147514\n",
      "Epoch: 44, Training Loss: 3.5467281341552734, Test Loss: 4.191626693263198\n",
      "Epoch: 45, Training Loss: 3.523172616958618, Test Loss: 4.180097127201582\n",
      "Epoch: 46, Training Loss: 3.4890592098236084, Test Loss: 4.167853692565301\n",
      "Epoch: 47, Training Loss: 3.462069272994995, Test Loss: 4.154752989007969\n",
      "Epoch: 48, Training Loss: 3.42140531539917, Test Loss: 4.148501290215386\n",
      "Epoch: 49, Training Loss: 3.397951364517212, Test Loss: 4.137973898589009\n",
      "Epoch: 50, Training Loss: 3.3819215297698975, Test Loss: 4.126166264216105\n",
      "Epoch: 51, Training Loss: 3.3494174480438232, Test Loss: 4.120743310812748\n",
      "Epoch: 52, Training Loss: 3.315197467803955, Test Loss: 4.105969604819712\n",
      "Epoch: 53, Training Loss: 3.282905340194702, Test Loss: 4.100490136580034\n",
      "Epoch: 54, Training Loss: 3.271838665008545, Test Loss: 4.093658772381869\n",
      "Epoch: 55, Training Loss: 3.2279837131500244, Test Loss: 4.083296433843747\n",
      "Epoch: 56, Training Loss: 3.2078678607940674, Test Loss: 4.073665773025667\n",
      "Epoch: 57, Training Loss: 3.1619057655334473, Test Loss: 4.0692685348819\n",
      "Epoch: 58, Training Loss: 3.129230499267578, Test Loss: 4.060806560998011\n",
      "Epoch: 59, Training Loss: 3.097099781036377, Test Loss: 4.057994864203713\n",
      "Epoch: 60, Training Loss: 3.06207013130188, Test Loss: 4.0553673276997575\n",
      "Epoch: 61, Training Loss: 3.0306177139282227, Test Loss: 4.049455033408271\n",
      "Epoch: 62, Training Loss: 3.005805015563965, Test Loss: 4.041532745264997\n",
      "Epoch: 63, Training Loss: 2.9883596897125244, Test Loss: 4.042055599617235\n",
      "Epoch: 64, Training Loss: 2.9693498611450195, Test Loss: 4.037382234226573\n",
      "Epoch: 65, Training Loss: 2.9702935218811035, Test Loss: 4.02551746609235\n",
      "Epoch: 66, Training Loss: 2.941394329071045, Test Loss: 4.018914027647539\n",
      "Epoch: 67, Training Loss: 2.9335598945617676, Test Loss: 4.010571402732772\n",
      "Epoch: 68, Training Loss: 2.9365835189819336, Test Loss: 4.005393969892252\n",
      "Epoch: 69, Training Loss: 2.907618761062622, Test Loss: 3.996884008850714\n",
      "Epoch: 70, Training Loss: 2.8955345153808594, Test Loss: 3.9876788938888397\n",
      "Epoch: 71, Training Loss: 2.874296188354492, Test Loss: 3.9835921056342847\n",
      "Epoch: 72, Training Loss: 2.8585736751556396, Test Loss: 3.9736026682034886\n",
      "Epoch: 73, Training Loss: 2.833449602127075, Test Loss: 3.971724115236841\n",
      "Epoch: 74, Training Loss: 2.8217685222625732, Test Loss: 3.9671309620443016\n",
      "Epoch: 75, Training Loss: 2.815049648284912, Test Loss: 3.959285850476737\n",
      "Epoch: 76, Training Loss: 2.7999014854431152, Test Loss: 3.949719041284889\n",
      "Epoch: 77, Training Loss: 2.78908109664917, Test Loss: 3.9451773997509116\n",
      "Epoch: 78, Training Loss: 2.7822582721710205, Test Loss: 3.9425128072199196\n",
      "Epoch: 79, Training Loss: 2.756122589111328, Test Loss: 3.9360237001168605\n",
      "Epoch: 80, Training Loss: 2.761733293533325, Test Loss: 3.9257079627778797\n",
      "Epoch: 81, Training Loss: 2.7540974617004395, Test Loss: 3.9218372780867297\n",
      "Epoch: 82, Training Loss: 2.7465829849243164, Test Loss: 3.9158690999252626\n",
      "Epoch: 83, Training Loss: 2.7258212566375732, Test Loss: 3.9146708153715037\n",
      "Epoch: 84, Training Loss: 2.715219020843506, Test Loss: 3.907568831636448\n",
      "Epoch: 85, Training Loss: 2.6881752014160156, Test Loss: 3.9087548496747258\n",
      "Epoch: 86, Training Loss: 2.6790101528167725, Test Loss: 3.906027457930825\n",
      "Epoch: 87, Training Loss: 2.6663079261779785, Test Loss: 3.9015958369380295\n",
      "Epoch: 88, Training Loss: 2.6531617641448975, Test Loss: 3.8983597418274543\n",
      "Epoch: 89, Training Loss: 2.6298818588256836, Test Loss: 3.898864744889616\n",
      "Epoch: 90, Training Loss: 2.6236510276794434, Test Loss: 3.892069717850348\n",
      "Epoch: 91, Training Loss: 2.6120588779449463, Test Loss: 3.887845170618308\n",
      "Epoch: 92, Training Loss: 2.5925395488739014, Test Loss: 3.8873858403677892\n",
      "Epoch: 93, Training Loss: 2.5696072578430176, Test Loss: 3.883686748417941\n",
      "Epoch: 94, Training Loss: 2.55873966217041, Test Loss: 3.885588031826597\n",
      "Epoch: 95, Training Loss: 2.551880359649658, Test Loss: 3.8791807745442246\n",
      "Epoch: 96, Training Loss: 2.5470080375671387, Test Loss: 3.8719534584970186\n",
      "Epoch: 97, Training Loss: 2.5273079872131348, Test Loss: 3.876846250861582\n",
      "Epoch: 98, Training Loss: 2.5262043476104736, Test Loss: 3.8725183395424274\n",
      "Epoch: 99, Training Loss: 2.5073509216308594, Test Loss: 3.873067855834961\n",
      "Epoch: 100, Training Loss: 2.4881720542907715, Test Loss: 3.8670919531523578\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "model = GraphSAGE(num_features=5, hidden_channels=16, num_classes=2).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()  # we use Mean Squared Error loss for regression tasks\n",
    "\n",
    "\n",
    "for epoch in range(100):  # run for 100 epochs\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)  # move batch to the device\n",
    "        optimizer.zero_grad()  # set gradients to zero\n",
    "        out = model(batch)  # forward pass\n",
    "        loss = criterion(out, batch.y)  # compute loss\n",
    "        loss.backward()  # backward pass (compute gradients)\n",
    "        optimizer.step()  # update model parameters\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            val_loss += criterion(out, batch.y).item() * batch.num_graphs\n",
    "\n",
    "    val_loss /= len(val_dataset)  # compute average validation loss\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {loss.item()}, Test Loss: {val_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss   - Static Baseline: 13.9304, Linear Baseline: 22.1829\n",
      "Validation Loss - Static Baseline: 11.4824, Linear Baseline: 20.2710\n"
     ]
    }
   ],
   "source": [
    "# Calculate static and linear baselines (formula on ANS)\n",
    "# THIS IS CORRECT! DO NOT CHANGE!\n",
    "\n",
    "def static_baseline(X):\n",
    "    return X[:, 1:3]  # initial x,y coordinates\n",
    "\n",
    "def linear_baseline(X):\n",
    "    return X[:, 1:3] + X[:, 3:5] * 5  # initial x,y coordinates plus velocity times time\n",
    "\n",
    "def compute_baseline_loss(baseline_fn, dataloader):\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.MSELoss()  # we use Mean Squared Error loss for regression tasks\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        predictions = baseline_fn(batch.x).to(device)\n",
    "        total_loss += criterion(predictions, batch.y).item() * batch.num_graphs\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "train_loss_static = compute_baseline_loss(static_baseline, dataloader)\n",
    "train_loss_linear = compute_baseline_loss(linear_baseline, dataloader)\n",
    "\n",
    "val_loss_static = compute_baseline_loss(static_baseline, val_dataloader)\n",
    "val_loss_linear = compute_baseline_loss(linear_baseline, val_dataloader)\n",
    "\n",
    "\n",
    " # now print out with filler spaces to make it easier to read\n",
    "print(f'Training Loss   - Static Baseline: {train_loss_static:0.4f}, Linear Baseline: {train_loss_linear:0.4f}')\n",
    "print(f'Validation Loss - Static Baseline: {val_loss_static:0.4f}, Linear Baseline: {val_loss_linear:0.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
